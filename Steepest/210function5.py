import numpy as np
from numpy import linalg as LA
epsilon = 10 ** -8
N = 1000
Max_LS_iter = 20
mu = 10 ** -4
v = 0.9
y = 10 ** -4


def function(num1, num2, c):
    return ((num1 - 1) ** 2) + ((num2 -1) ** 2) + (c *((num1 ** 2) + (num2 ** 2) - 0.25))
def gradient(num1, num2, c):
    a = ((4 * (num1 ** 3) * c) + (4*c * num1 * (num2 ** 2)) + (2 * num1) - (num1 * c)) - 2
    b = ((4 * (num2 ** 3) * c) + (4*c * (num1 ** 2) * num2) + (2 * num2) - (num2 * c)) - 2
    return np.array([a,b])

def hessian(num1, num2, c):
    a = (12 * (num1 ** 2) * c) + (4 * c * (num2 ** 2)) + 2 - c
    b = 8 * c * num1 * num2
    c = 8 * c * num1 * num2
    d = (12 * (num2 ** 2) * c) + (4 * c * (num1 ** 2)) + 2 - c
    return np.array([[a,b],[c,d]])




def backtracking(mu, p_k, x_k, Max_LS_iter=20):
    n = 0
    alpha = 1
    arg1 = np.add(x_k, (alpha * p_k))
    arg2 = x_k
    while function(arg1[0], arg1[1],c) > (function(arg2[0], arg2[1], c) + (mu * alpha) * np.matmul(np.transpose(gradient(arg2[0], arg2[1], c)), p_k)) and n <= Max_LS_iter:
        alpha = alpha / 2
        n += 1
        print(n)
    return alpha

def gradient_descent(x0, c, epsilon=10 ** -8, N=1000, Max_LS_iter=20, mu=10 ** -4, v=0.9, y=10 ** -4):
    data = [['iteration', 'norm of gradient', 'step size', 'x_k']]
    k = 0
    x_k = x0
    while (LA.norm(gradient(x_k[0], x_k[1], c),2) / (1 + abs(function(x_k[0], x_k[1], c)))) > epsilon and k <= N:  #figure out gradient information on how to plug stuff into it
        p_k = -gradient(x_k[0], x_k[1], c)
        alpha = backtracking(mu, p_k, x_k)
        x_k = x_k + (alpha * p_k)
        k += 1
        data.append([k, LA.norm(gradient(x_k[0], x_k[1], c)), alpha, x_k])
    if k > N:
        print("Number of iterations exceeded limit: ", N)
        return data
    else:
        return data

x0 = np.array([1,-1])
c = int(input("Enter C Value: "))
data = gradient_descent(x0, c)
if len(data) > 15:
    a = data[0:11]
    a.extend(data[len(data) - 6:len(data) - 1])
    for item in a:
        print(item)
else:
    for item in data:
        print(item)

# c = 1
# Number of iterations exceeded limit:  1000
# ['iteration', 'norm of gradient', 'step size', 'x_k']
# [1, 13.038254062348027, 4.76837158203125e-07, array([ 0.99999666, -0.99999475])]
# [2, 13.038103317382422, 4.76837158203125e-07, array([ 0.99999332, -0.99998951])]
# [3, 13.037952575508402, 4.76837158203125e-07, array([ 0.99998999, -0.99998426])]
# [4, 13.03780183672587, 4.76837158203125e-07, array([ 0.99998665, -0.99997902])]
# [5, 13.03765110103475, 4.76837158203125e-07, array([ 0.99998331, -0.99997377])]
# [6, 13.037500368434943, 4.76837158203125e-07, array([ 0.99997997, -0.99996853])]
# [7, 13.037349638926369, 4.76837158203125e-07, array([ 0.99997664, -0.99996328])]
# [8, 13.037198912508936, 4.76837158203125e-07, array([ 0.9999733 , -0.99995804])]
# [9, 13.037048189182558, 4.76837158203125e-07, array([ 0.99996996, -0.9999528 ])]
# [10, 13.036897468947146, 4.76837158203125e-07, array([ 0.99996662, -0.99994755])]
# [996, 12.889777438651, 4.76837158203125e-07, array([ 0.99669866, -0.99480297])]
# [997, 12.889629726988504, 4.76837158203125e-07, array([ 0.99669537, -0.99479778])]
# [998, 12.889482018331947, 4.76837158203125e-07, array([ 0.99669207, -0.99479259])]
# [999, 12.889334312681248, 4.76837158203125e-07, array([ 0.99668878, -0.9947874 ])]
# [1000, 12.889186610036317, 4.76837158203125e-07, array([ 0.99668549, -0.9947822 ])]

# c = 10
# Number of iterations exceeded limit:  1000
# ['iteration', 'norm of gradient', 'step size', 'x_k']
# [1, 101.85139033803509, 4.76837158203125e-07, array([ 0.99996662, -0.99996471])]
# [2, 101.84013009648653, 4.76837158203125e-07, array([ 0.99993325, -0.99992943])]
# [3, 101.82887190080982, 4.76837158203125e-07, array([ 0.99989988, -0.99989415])]
# [4, 101.81761575048569, 4.76837158203125e-07, array([ 0.99986651, -0.99985888])]
# [5, 101.8063616449952, 4.76837158203125e-07, array([ 0.99983314, -0.99982361])]
# [6, 101.79510958381952, 4.76837158203125e-07, array([ 0.99979978, -0.99978834])]
# [7, 101.78385956643997, 4.76837158203125e-07, array([ 0.99976643, -0.99975308])]
# [8, 101.7726115923381, 4.76837158203125e-07, array([ 0.99973308, -0.99971782])]
# [9, 101.76136566099554, 4.76837158203125e-07, array([ 0.99969973, -0.99968256])]
# [10, 101.75012177189414, 4.76837158203125e-07, array([ 0.99966638, -0.99964731])]
# [996, 91.58055267946231, 4.76837158203125e-07, array([ 0.96851497, -0.96664581])]
# [997, 91.5710965487096, 4.76837158203125e-07, array([ 0.96848503, -0.96661402])]
# [998, 91.56164202109049, 4.76837158203125e-07, array([ 0.96845509, -0.96658223])]
# [999, 91.55218909622538, 4.76837158203125e-07, array([ 0.96842516, -0.96655045])]
# [1000, 91.54273777373496, 4.76837158203125e-07, array([ 0.96839523, -0.96651867])]



# c = 100
# Number of iterations exceeded limit:  1000
# ['iteration', 'norm of gradient', 'step size', 'x_k']
# [1, 991.6925802629473, 4.76837158203125e-07, array([ 0.99966621, -0.99966431])]
# [2, 990.6051651359286, 4.76837158203125e-07, array([ 0.99933279, -0.99932898])]
# [3, 989.5196996005667, 4.76837158203125e-07, array([ 0.99899974, -0.99899402])]
# [4, 988.4361787780163, 4.76837158203125e-07, array([ 0.99866706, -0.99865943])]
# [5, 987.3545978051081, 4.76837158203125e-07, array([ 0.99833473, -0.9983252 ])]
# [6, 986.2749518342863, 4.76837158203125e-07, array([ 0.99800278, -0.99799134])]
# [7, 985.1972360335482, 4.76837158203125e-07, array([ 0.99767118, -0.99765784])]
# [8, 984.1214455863827, 4.76837158203125e-07, array([ 0.99733995, -0.99732471])]
# [9, 983.0475756917104, 4.76837158203125e-07, array([ 0.99700908, -0.99699194])]
# [10, 981.9756215638223, 4.76837158203125e-07, array([ 0.99667858, -0.99665953])]
# [996, 431.8746662330927, 4.76837158203125e-07, array([ 0.78244525, -0.78074108])]
# [997, 431.59298623952736, 4.76837158203125e-07, array([ 0.78230043, -0.78059467])]
# [998, 431.3116023423929, 4.76837158203125e-07, array([ 0.7821557 , -0.78044836])]
# [999, 431.030514107911, 4.76837158203125e-07, array([ 0.78201107, -0.78030214])]
# [1000, 430.7497211031186, 4.76837158203125e-07, array([ 0.78186654, -0.78015601])]